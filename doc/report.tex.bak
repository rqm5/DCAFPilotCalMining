%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[final, 12pt]{elsarticle}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\ps@pprintTitle}{\footnotesize\itshape
  Preprint submitted to \ifx\@journal\@empty Elsevier
  \else\@journal\fi\hfill\today}{\relax}{}{}
\makeatother

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

% for pdf outlines/bookmarks from section titles 
\usepackage[bookmarks,bookmarksopen,bookmarksdepth=3]{hyperref}
\hypersetup{pdftex,colorlinks=true,allcolors=blue}
\usepackage{hypcap}

% for bibi
% \usepackage{biblatex}

% for plotting diagrams by TKiz
\usepackage[latin1]{inputenc}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{caption}
\newcommand*{\h}{\hspace{5pt}}% for indentation
\newcommand*{\hh}{\h\h}% double indentation


%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\begin{document}

\begin{frontmatter}

  %% Title, authors and addresses

  \title{Using Conference Schedules for Dataset Usage Prediction}

  %% use the tnoteref command within \title for footnotes;
  %% use the tnotetext command for the associated footnote;
  %% use the fnref command within \author or \address for footnotes;
  %% use the fntext command for the associated footnote;
  %% use the corref command within \author for corresponding author footnotes;
  %% use the cortext command for the associated footnote;
  %% use the ead command for the email address,
  %% and the form \ead[url] for the home page:
  %%
  %% \title{Title\tnoteref{label1}}
  %% \tnotetext[label1]{}
  %% \author{Name\corref{cor1}\fnref{label2}}
  %% \ead{email address}
  %% \ead[url]{home page}
  %% \fntext[label2]{}
  %% \cortext[cor1]{}
  %% \address{Address\fnref{label3}}
  %% \fntext[label3]{}


  %% use optional labels to link authors explicitly to addresses:
  %% \author[label1,label2]{<author name>}
  %% \address[label1]{<address>}
  %% \address[label2]{<address>}

  \author{Ting Li}

  \address{Advisor: Valentin Kuznetsov}
  % \address{Cornell University}

%  \begin{abstract}
    %% Text of abstract
%  \end{abstract}

\end{frontmatter}

%% main text
\section{Overview}
\label{S:1}

%% - problem statement
%% - your task goals

The Compact Muon Solenoid (CMS) is a large general-purpose particle physics detector, built on the Large Hadron Collider (LHC) at CERN in Switzerland and France. The goal of the CMS experiments is to investigate a wide range of physics, including the search for the Higgs boson, extra dimensions, and particles that could make up dark matter \cite{web:wiki}.
The CMS experiments present challenges not only in terms of the physics to discover and the detector to build and operate, but also in terms of the data volume and the necessary computing resources. Data sets and resource requirements are at least an order of magnitude larger than in previous experiments \cite{web:cms}.
The CMS computing system relies on a distributed infrastructure of Grid resources, services and toolkits, to cope with computing requirements for storage, processing and analysis of data provided by the experiments. 

It will be beneficial if we can reliably predict the usages of the datasets used in the future experiments, so the data management staff can make enough replicates of the datasets, and deploy them to the storage centers nearby, and thus increase the throughput of data delivery to the researchers. 

An approach to the prediction is to binarize the dataset access count in unit time (say, a week) into popularity labels, so that the problem becomes a classification one, and use the attributes from the CMS dataset access data as features. \cite{web:vk}
The approach can make sensible prediction in some experiments.
The IT engineers, however, are facing ever increasing demand of data delivery, so they continue on searching for ways to achieve better prediction performance.

My project task is to study if and how the CMS conference schedules can be useful for predicting CMS dataset future popularity.
This project is supervised by Valentin Kuznetsov. I thank him for his guidance and patience.


\subsection{Workflow and Design of the Project}

My modeling and analysis proceed in two directions: analyze the time series of the weekly conference count and of the weekly access counts to each dataset,  and evaluate weekly conference counts as new features added to the classification model.

The diagrams \ref{chart1} and \ref{chart2} are the flowcharts of the simplified processes in the two types of modeling and analysis in this project.
In the diagrams, program files are shown without being bounded by boxes, and the inputs, intermediates, and outputs of programs are shown inside boxes.
The inputs, intermediates, and outputs of a program are simplified, for example, there are much more dataset access files than the two shown for two weeks, and some programs also takes auxiliary inputs besides those shown in the diagrams.
The purpose of making the diagrams is to show the design and workflow of the project in a big picture.

\input{time_series_flow.tex}
\input{data_flow.tex}

The programs which I have written are the following. They are written in Python, and run under \verb|Python| 2.6.8 on 64-bit Scientific Linux CERN 6 (SLC6) on the \verb|lxplus.cern.ch| server \cite{web:lxplus}. I made them  available on GitHub \cite{web:tlgithub}. Here I include brief descriptions of the programs:

\begin{itemize}
\item \verb|cms_conf_parser.py|
  
  It parses the conference data dump file into a conference count time series.

  Input:

  \verb|--indump|: a csv.gz file for conference data dump from a database

  \verb|--inschema|: a plain text file for the schema of the conference data dump file

  Output:

  \verb|--outdir|: a directory for csv.gz files for conference count per week time series, for conference count for future weeks, and for parsed conference records
  
  
\item \verb|select.py|

  It selects records from dataset access files whose attribute is some value.

  Input:
  
  \verb|--indir|: a directory containing the csv.gz files for the input original dataset access data

  \verb|--attr|: an attribute to select by

  \verb|--attrval|: a value of the attribute to select by

  Output:

  \verb|--outdir|: a directory cotaining csv.gz files for the output selected records

\item \verb|merge_access_conf.py|

  It add records from conference counts to dataset access records

  Input:

  \verb|--indir|: a directory containing the csv.gz files for the input dataset access data

  \verb|--inconf|: a csv.gz file for the input conference count data

  Output:

  \verb|--outdir|: a directory containing csv.gz files for the output merged data 

\item \verb|time_series.py|

  It generates time series for each dataset from the dataset access data, and analyze the cross correlations and seasonalities for the time series of dataset access and time series of conference count.

  Input:
  
  \verb|--indir|: a directory containing csv.gz files for the input dataset access data

  \verb|--inconf|: a csv.gz file for the input conference count data

  Output:

  \verb|--outdir|: a directory containing csv.gz files for the output time series of each dataset, and image files for the plots of cross correlation and FFT of the time series

\end{itemize}


The other programs were written by Valentin \cite{web:vkgithub}, which were part of package \verb|DCAFPilot| (version 0.0.42). I slighly modified his \verb|model.py| for  adding independence tests between each feature and the outcome, changing the split ratio of train and validation sets, fixing the random seeds, and changing the number of feature in the feature importance output.

\section{Preparing Datasets}
\label{S:2}
% - produced dataset (how you generate it and where it is stored)

\subsection{CMS Conference Data}


I receive the CMS conference data in a file, which was dumped from querying a database.
It contains a collection of conference records.
Each record represents a conference, and consists of the conference's ID, name, cateogry, description, held time, held location, website link, etc.

I perform the following processings on the conference data dump file, to generate the data needed for modeling and analysis:

\begin{itemize}
  
\item In the conference data file, each record occupies multiple lines, and records are separated by a blank line, the fields of a record are separated by a comma or a new line, and each field may have comma(s) inside or may even be empty (that is, missing).
Thus we need to  Parse the text in the conference data file, into a list of dictionaries in Python, with each dictionary representing a conference record.
I perform the parsing according to the conference records' schema, which provide each attribute's name, type, and length when it is the string type.
The parsing of the dump file according to the schema is implemented by regex pattern matching in \verb|cms_conf_parser.py|.

\item After parsing, we count the conferences that are held in each week, and obtain a time series of weekly conference count over the weeks from the beginning of 2006 to mid September 2015 (505 weeks in total).  
This is implemented in \verb|cms_conf_parser.py|, by grouping the conference records by week, which is in turn implemented by creating a dictionary, with a key being a week, and its value being a list of conference records falling into that week.
Note that a week here is not a calendar year. The first day of a year is always the start of the first week of the year, and the last week of a year may contain more than 7 days.

\end{itemize}


\subsection{CMS Dataset Data}

I receive the CMS dataset data as a collection of files.
Each file stores the records of datasets accessed in a week. In such a file, each record is for a dataset, and the fileds of a record are the information about the record's dataset (such as its id, size) and the usage of the dataset during the file's week (such as number of its accesses and cpu time of its accesses in the week). \cite{web:vk} has explanations for the attributes of the records.
The timestamp week for a file is not in the file's content, but is coded in the file's name, for example, \verb|dataframe-20130101-20130107.csv.gz|.

The files are easier to parse  than the files for conference data, because their contents are organized in a csv file format: 
each record occupies a line, the records are separated by a newline character, and the fields of a record are spearated by a comma.

I perform the following processings on the dataset access files, to generate the data needed for modeling and analysis:

\begin{itemize}
  
\item Select records for datasets belonging to a tier.
This is implemented by list comprehension in \verb|select.py|.
Tiers are levels in the hierachy of CMS distributed computing infrastructure. 
There are certain tiers whose sites store datasets dedicated to software testing, and the access to those datasets is unrelated to their access by physicists. Thus those  datasets are of little interest to our project, and we focus only on datasets from Tier 2 (reconstructed data) and Tier 3 (analysis data).
In this report, the datasets used for analysis and modeling come from Tier 2, and modeling and analysis for datasets from Tier 3 are similar.

\item For each dataset, group its records from all the dataframe files, and extract weekly dataset access count and output a time series of weekly access count over the weeks when the dataset has records.
This is implemented in \verb|time_series.py|.
The grouping is implemented by using a dictionary, with each key being a dataset and its value being a list of the records of the dataset.
For a record, the dataset it belongs to is identified by the \verb|dataset| and \verb|dbs| attributes,
and the access count to the record's dataset in the record's week is the value of the \verb|naccess| attribute.


\end{itemize}

\subsection{Add Conference Counts to Dataset Access Records in the same week}

For modeling the problem as a classification one in a later section, we merge the conference data to the dataset data.
To each dataset access record, I add the following new fields: the conference counts in the same week as the dataset access record, and the accumulated conference counts up to some future weeks.
This needs to find the corresponding conference count for a given week, which is implemented by matching the timestamp of a dataset access record to the timstamp of a conference count, again a string pattern matching problem.
This is implemented in \verb|merge_access_conf.py|.

\section{Time Series Analysis}

%% - plots confirming the study
%% - metrics you choose
%% - description of analysis
%% - analysis results


As mentioned in the Overview section,
my task is to study whether and how conference data can be used, possibly with CMS dataset data, for predicting future dataset usage.
My study has been going in two directions, which differ in whether and how we take into account the timestamps of the CMS dataset records and of conference schedules.

In the first direction, I analyze the relation between two time series indexed by weeks: conference count per week, and an individual dataset's access count per week (given as values of the \verb|naccess| attribute from CMS dataset records).

Note that:

\begin{itemize}

  \item
There are 3279 datasets in Tier 2, but I do not consider them all for the following reasons.

Different datasets have different numbers of records.
For time series analysis purpose, I only consider those datasets which have more than $10$ records. There are $237$ of them.
The maximum length of a dataset is $103$, the minimum is $1$, and the median is  $5$, and the standard deviation is $5.59$.
The histogram of dataset lengths is in Figure \ref{len}.
 
\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../data/tier/2/datasets/dataset_3279_lengths_hist.pdf}
\end{center}
\caption{Histogram of lengths of dataset access count series}
\label{len}
\end{figure}
   
 
The \verb|naccess| attribute of some datasets are constant (for example, $0$), which makes cross-correlation become invalid, and Fourier transform to be a single spike at frequency $0$. Prediction of \verb|naccess| of such datasets is also trivial. Therefore I remove such datasets.
There are $2973$ datasets whose \verb|naccess| attributes are constant, and $2959$ of them are constantly $0$.
The maximum, minimum, median and standard deviation of variances of \verb|naccess| attribute values among the datasets are $826207739.67, 0, 0$, and $22326593.86$.

After filtering datasets by the above two criterions, $125$ datasets remain for the time series analysis.

\item
The records of a dataset can be missing for some weeks between its existing records in other weeks. For the purpose of time series analysis, I deal with the missing values by filling with $0$ for \verb|naccess| in missing weeks.

\end{itemize}

\subsection{Seasonality}

It is our original guess that conference schedules and dataset access may expose seasonalities or periodicities, due to holidays and vacations.
If it were true, we then would like to use their seasonalities or periodicities to simplify our modeling.

It is not obvious to identify seasonality in either the plot of the conference count series  or an arbitrary dataset access series however.
An alternative way to study seasonality in a time series is to calculate discrete Fourier transform on the time series by the fast Fourier transform algorithm, and search for significant spikes that represent the frequencies of seasonalities.
I calculate the FFT of a time series using numpy.fft.rfft().
Existence of noises in a time series usually leads to spurious spikes in its DFT series. Picking out the highest spikes requires smoothing the series with trial and error.
Thus I prefer visually checking over automatically choosing the highest spikes.


Figure \ref{cffft} is the plot of the DFT of the conference count series, and  \ref{dsfft1} and \ref{dsfft2} the DFT of some datasets' access count series.

\begin{figure}
\begin{center} 
\includegraphics[scale=0.5]{../data/tier/2/datasets/conf_ct_perweek_505fft.pdf}
\end{center}
\caption{DFT of conference count per week time series}
\label{cffft}
\end{figure}

In the DFT plot \ref{cffft} of the conference count series, the highest spike occurs at the first frequency (that is, frequency $1/505$, where $505$ is the length of the time series in weeks) , which does not imply periodicity, because it corresponds to the whole time domain of the time series ($505$ weeks from the beginning of 2013 to the middle of September 2015). The second highest spike occurs at the 10th fequency (that is, frequency $10/505$), which corresponds to a time period of $505/10 = 50.5$ weeks, roughly a year. This confirms our intial guess of seasonality due to holidays. Since the spike at the first frequency is so dominant, and the second highest spike is not much higher than the other spikes, the time series itself exhibit only slightly periodicity from the second highest spike.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../data/tier/2/datasets/471188_1_116_fft.pdf}
\end{center}
\caption{DFT of access count per week time series  of a dataset}
\label{dsfft1}
\end{figure}


In the DFT plot \ref{dsfft1} of the access series of the dataset (471188,1), the highest spike occurs at the 8th frequency (that is, frequency $8/116$, where $116$ is the length of the time series in weeks), which corresponds to a time period of $116/8 = 15$ weeks. This coincides with the observation of a periodicity between 10 and 20 weeks from the plot of the time series itself.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../data/tier/2/datasets/701364_2_116_fft.pdf}
\end{center}
\caption{DFT of access count per week time series of a dataset}
\label{dsfft2}
\end{figure}


In the DFT plot \ref{dsfft2} of the access series of the dataset (701364,2), there is no spike significantly higher than the others. So there seems no significant seasonality in the dataset access series.

The DFT plot varies from dataset to dataset, and there seems not guruantee to find seasonality in them.

Neither is it clear yet how to relate the seasonalities of the dataset access series with that of the conference count series.
If both the conference count series and an individual dataset access count series are both periodic,
we may limit our study to a time interval whose length is the common least multiple of their periods, instead of the entire time intervals where the two time serires were created/defined.


\subsection{Cross Correlation}

I calculate the cross correlation between  an individual dataset's access count series and the conference count series, over lags ranging between $-90$ and $90$ with step $1$. At each lag, I call \verb|scipy.stats.stats.pearsonr()| to calculate a cross correlation, and besides a cross correlation, it also returns a p-value for testing a null hypothesis that the two series are uncorrelated at the lag.

The plots \ref{cor1}, \ref{cor2} and \ref{cor3} are the cross correlation between the access count series of some exemplar datasets and the conference count series.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../data/tier/2/datasets/471188_1_365.pdf}
\end{center}
\caption{Cross correlation between the conference count series and the access count series of a dataset}
\label{cor1}
\end{figure}

\begin{figure}
\begin{center}   
\includegraphics[scale=0.5]{../data/tier/2/datasets/701364_2_364.pdf}
\end{center}
\caption{Cross correlation between the conference count series and the access count series of a dataset}
\label{cor2}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../data/tier/2/datasets/7686105_0_372.pdf}
\end{center}
\caption{Cross correlation between the conference count series and the access count series of a dataset}
\label{cor3}
\end{figure}


The plots show that the lag where the highest cross corrleation in magnitude occurs varies with different datasets, and can be either positive lags (for example, the plot for datasets \verb|(471188,1)| and \verb|(701364,2)|), or negative lags (for example, the plot for dataset \verb|(7686105,0)|).
Cross correlation peaking at a positive lag means that future conference schedules can lead the current dataset access, while peaking at a negative lag means that past conferences can still have residual influence on the current dataset access.


The plot \ref{laghist} is the histogram of such lags with p-values smaller than 0.05 (indicating the uncorrelated null is rejected with significance level 0.05). It shows that cross correlation achieves its maximum most probably around $75$ lags in weeks (roughly one year and a half), and with decreasing frequencies at the lags  $65, 85, 40, 50, 60$, and $5$:
 
\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../data/tier/2/datasets/lag_144_hist.pdf}
\end{center}
\caption{Histogram of lags with maximum cross correlation}
\label{laghist}
\end{figure}

For a given dataset, we can build a prediction model, by regressing the dataset weekly access count on conference counts in some future weeks away by the lags chosen from the cross correlation analysis.
There are potential drawbacks of such modeling:

\begin{itemize}
  
\item We assume the relation between dataset access count and future conference counts are the same over the time, which is something like or similar to stationarity: the distribution of the dataset weekly access count given the future weeks' conference counts is the same across the weeks. After building such models, we lose track of time.
Without the stationary assumption, it remains to build a forecast model that can capture the nonstationary in the relation between the two time series.
In any case, it will be good to test if the stationary assumption holds up front.

\item It does not make use of the other attributes from CMS dataset data, except using the dataset access count as the outcome.

\end{itemize}

\section{Classification}

In this section, we consider a different direction from time series analysis.

We simplify the quantitative outcome, that is, the weekly dataset access count, to a qualitative popularity class label, by binarizing the counts at a threshod 100.
% choose threshold for output target? by seeing histogram of the output target in train set, and by requirement such as how much percentage of dataset should be positive at a time (not necessarily 50%) (since he set it to be 100, there must be a reason? so don't change the threshold?)?
This changes the problem from a regression one to a classification one, potentially increasing the generalization ability of the trained model.
% try regression instead of classification?
% is the high dim of features may be a problem more to regression than to classification? but for random forest regressor?

Moreover, we make the following stationary assumption:
within a certain period, the conditional distribution of the dataset popularity binary outcome on the features (that is, future conference counts and other dataset attributes from CMS dataset data, see below) changes little over time, and can be approximately seen as being stationary.
So we drop the timestamps from the data after combining the conference counts and the dataset access records.
For example,
we assume the length of such a period can be around one year (about 52 weeks), and choose to train on the data from the May 2013 to April 2014, and test on the data in the following week after April 2014.

 
% - Assume all the datasets follow the same model. So we further drop the dataset id (\verb|dataset|, \verb|dbs|).
% he uses dataset id as features (categorial features) for training RF classifier.

\subsection{Candidate Features}

When choosing the candidate features to be considered in the classification problem, I would like to consider all that are available, and might later perform feature selection as needed.

The candidate features are the future conference counts weeks away by lags chosen from the cross correlation analysis of time series, and the attributes in CMS dataset data that are available at the time of prediction.

We consider the future conference counts as candidate features, because we suspect that the future conference shedules may influence current dataset access counts, and they are available for predicting accesses to the datasets.

Not all attributes from the dataset access records have their values available before the time to predict for.
For example,
\verb|naccess| is what we generate the binary outcome directly from, so it must obviously be dropped.
So are \verb|nusers| (the number of users $*$ days to a dataset, reported by PopularityDB), and \verb|totcpu| (the number of cpu hours to accessed dataset, reported by PopularityDB).
Valentin  singled them out in his demo examples of using his programs.
% manual feature selection, which depends on relation to outcome:
% does cpu and proc_events relate to naccess, and should be dropped? they both have high importance in random forest.
% id is dropped because specified as id to model?
% dataset and dbs are both categorical, and useful for classifying popular and unpopular.

The conference count per week can change dramatically, so instead of using them directly, I use more stable features which are the accumulated future conference counts up to $1, 2, 4, 6, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65$, and $70$ weeks. The number of the weeks are chosen somehow based on the lags at which the cross correlation between conference count series and dataset access count series is high (see the previous section).


\subsection{Feature Types}

Classifier, feature transformation, feature selection and ranking may work with some feature types (categorical, ordered, numerical, ...), and not with other feature types.

Some features from CMS dataset data seem to be categorical, for example, \verb|dataset|, \verb|dbs|, and \verb|rel*|.
In the dataset access files, however, all the features (including categorical ones) are coded in numerical values.
This can be problematic if not handled correctly.
  
Decision trees and therefore random forest can take categorical features. %esplly for binary outcome, c.f. hastie $9.2.4
Scikit-Learn does not directly support categorical features in trees or forests however \cite{web:slcat}. % how to specify a feature is categorical instead of numerical to scikit learn algorithms?
If categorical features are coded as numbers, which imposes a non-existing ordering on the categorical values,
the random forest training algorithm will rely on that ordering,  and the trained random forest classifier will be misleading in terms of interpretation and have more overfitting to the training data.
% Feature importances calculated in random forest classifier may not work for numerically coded categorical features?

It may also be misleading to apply numerical feature transformation (such as standardization of features  to have zero mean and unit variance) on  categorical features that are numerically coded.
The random forest training algorithm theoretically does not depend on standardization, because standardization does not change the ordering between the feature values.
If we use random forest for classification, it seems better not to do standardization in preprocessing. (When I set  \verb|scaler=None| (or just not specify the option) to the program \verb|model.py|, the program will not predict on the test set.)
% Q: how to pass none to scaler. in "model.py", there is a test "if opts.scaler". 


\subsection{Ranking Sets of Features}

We can compare different sets of features, by using them to train classifiers, and comparing the classifiers' test performances.
Here I train random forest classifiers by calling \verb|model.py|, which in turn calls the learning algorithm implemented in the Scikit-Learn library by the class \verb|sklearn.ensemble.RandomForestClassifier|.

We are intersted more in the popular class (that is, the positive class) than the unpopular one (that is, the negative class), so we choose classification performance measures that more focus on the positive class:  accuracy, precision, recall and $F1$ scorers.

Note:

\begin{itemize}
  
\item I use the default complexity settings of a random forest classifier based on the empirical recommendations in the library's user guide.
In the default settings,
a random forest has 10 random trees, 
each tree is allowed to be contructed without constraints on its complexity (depth), and
the number of the candidate features randomly chosen at each node split is the square root of the number of all the features.

Thanks to the randmization in constructing each tree, and the averaging of the probabilistic predictions of all the trees, 
the tendency of a random forest classifier to overfit is generally small. % more immune to curse of dimensions, that is, can work with  many features?
Thus I skip the validation step to tune the complexity of the forest, and then the usage of validation set is not necessary.
Also because validation reduces the actual training set, I skip validation by setting the validation set's percentage \verb|split=0| in \verb|model.py|.

\item To make the experiments reproducible, I fix the random seeds in the program \verb|model.py|, which appear in
the \verb|random_state| parameter to both the forest constructor function \verb|sklearn.ensemble.RandomForestClassifier()|, and to the splitting datset function \verb|train_test_split()|.
Since I skip validation by setting the validate size \verb|split=0|,  the \verb|random_state| parameter to  \verb|train_test_split()| does not matter.
% the output is random, although the random_state parameter is set to be fixed in model.py "setattr(clf, "random_state", 123)". why?
% he miss the random_state param for  train_test_split(). 

\end{itemize}


% I modify model.py to output importance for all features, not just the first 9 most important features


%% \subsubsection{}

%% - compare the classification performances with different sets of features

%% train a random forest classifier with different sets of features, and compare them more easily?
%% - conferent counts alone (with dataset id  (dataset, dbs) from CMS dataset data, which I want to skip but don't figure out how to in the program \verb|model|)
%% - the attributes in CMS dataset data.
%% - combination of all the above features.

\subsection{Ranking Individual Features}

There are different ways to rank individual features in a classification problem.


\subsubsection{Importance Measurements Returned by Random Forest Classifier}
% instead of using random forest,  check if conf count is related or important to the classification or regression, by some other ways?

When a random forest classifier make predictions on a test set,
the relative rank (that is, depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. \cite{web:slguide}

Note that a random forest classifier trained from a train set has already arranged the features at each node based on the train set, so the calculation of feature importance by the trained random forest classifier should be on a different data set (here a test set).

In an object of the class \verb| sklearn.ensemble.RandomForestClassifier|, the attribute \verb|feature_importances_| stores the importances of the features on a test set.

\subsubsection{Feature Ranking by Independence Testing}

Alternatively, we can rank the features, by testing the independence between each feature and the popularity outcome, and then ordering their p-values (the smaller the p value for testing a feature is, the more confidence we will have to reject the null hypothesis of independence between the feature and the outcome).

I add to \verb|model.py| two functions from the Scikit-Learn library to perform the independence tests:
 \verb|sklearn.feature_selection.chi2()|, which implements the Chi-squared independence test, and \verb|sklearn.feature_selection.f_classif()|, which implements the ANOVA F test.

Some notes:

\begin{itemize}

  \item Since the independence tests are not part of the random forest learning algorithm, we can perform the tests on the training set.

  \item ANOVA F test can only work between a numerical feature and a categorical outcome, because it calculates the sample mean and variance of a feature.
So when applying the test for categorical features that are numerically coded, the p-values returned may be unreliable.

% chi square independent test
% I think the chi squared test function work for all feature types, although it says it only work for nonnegative features.
% if it is correct,
% Feature transformations, such as standardizing features to have zero mean and unit variance, will transform the features to have negative values. That makes feature selection tests (for example, chi square test) not applicable.
% Therefore I apply the test of feature independece before standardizing features, or do not transform features.

\end{itemize}


\subsection{Experiments and Analysis}
 
In the output of running the \verb|model.py| and \verb|check_prediction.py| on all the candidate features (see \ref{app1}),
the two independence tests show that

\begin{itemize}

  \item
Future conference counts accumulated more than 40 weeks mostly have p-values less than $0.05$,
while those within 40 weeks mostly have p-values bigger than $0.05$.
This means that future conference counts likely rank higher if they are accumulated into futher future.
However it is hard to use conference data that are more into the future (greate than $70$ weeks), because the timestamp-matched records between the CMS dataset data and the CMS conference data are between Jan 2013 and May 2015, and I have used the data from May 2013 to April 2014 in the experiment.

\item
  Most of the attributes from the CMS dataset data rank higher than most of the future conference counts.
This implies that the future conference counts are not as important to the classification problem as most attributes from CMS dataset data.

 \end{itemize} 


The feature importances from the trained random forest classifier also show the similar things, and some further observations are:

\begin{itemize}
  
\item   
Some of \verb|rel2_N|, \verb|rel3_N| features are the most important.
These features represents releases of datasets, and therefore it is reasonable that datasets with longer release histories are more likely on high demand, and therefore will be more popular in the future as well.

\item
\verb|cpu| and \verb|proc_evts| are also important.
If \verb|cpu| and \verb|proc_evts| are measured only after datasets are accessed, then they may not be available at the time of predicting dataset future popularity, and should be dropped as well. (They were not dropped in the demo example.)

\end{itemize}


The performance of the trained random forest classifier on the test set is almost perfect.
\begin{verbatim}
  Score metric (accuracy_score): 0.993377483444
  Score metric (precision_score): 1.0
  Score metric (recall_score): 0.909090909091
  Score metric (f1_score): 0.952380952381
\end{verbatim}


For comparison, when the features are only those from the CMS dataset access data, the performace is perfect (see  \ref{app2}):
\begin{verbatim}
Score metric (accuracy_score): 1.0
Score metric (precision_score): 1.0
Score metric (recall_score): 1.0
Score metric (f1_score): 1.0
\end{verbatim}


When the features are only the future conference counts (plus \verb|dataset| and \verb|dbs|, which must be used by \verb|model.py| and cannot be dropped), the performace falls back somehow (see \ref{app3}):
\begin{verbatim}
Score metric (accuracy_score): 0.960264900662
Score metric (precision_score): 0.777777777778
Score metric (recall_score): 0.636363636364
Score metric (f1_score): 0.7
\end{verbatim}
The output also show that the trained random forest classifier ranks \verb|dataset| and \verb|dbs| higher than the future conference counts.
This again implies that the future conference counts are not as important as the attributes from the dataset access data.


\section{Conclusion}

In this project, we study the influence of the future conference counts on prediciting the dataset popularity.

The experiments show that the future conference counts can provide insights into the problem and data,
but are not as important to the classification as many attributes from CMS dataset data.

Changing the way to use conference counts in the classification problem, or changing to a modelling method that is different from classification (such as modelling the dataset access count series on its own right, modelling the relation between the outcome series and feature series, or regression between the outcome and the features) may be worth exploration.



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix

\section{Output when using both future conference counts and dataset access attributes as features}
\label{app1}
\input{output_conf+access_novalid.tex}

\section{Output when using only dataset access attributes as features}
\label{app2}
\input{output_access_novalid.tex}

\section{Output when using mostly future conference counts as features}
\label{app3}
\input{output_conf_novalid.tex}

%% 
  
%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

\section*{\refname} \addcontentsline{toc}{section}{\refname}
\bibliographystyle{model1-num-names}
\bibliography{sample.bib}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}



\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.
